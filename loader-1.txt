{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "####\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import os"
      ],
      "metadata": {
        "id": "9n1--i7aC0Sp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_paths):\n",
        "    dataframes = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            file_extension = os.path.splitext(file_path)[1][1:].lower()\n",
        "\n",
        "            if file_extension == 'csv':\n",
        "                df = pd.read_csv(file_path)\n",
        "            elif file_extension in ['xls', 'xlsx']:\n",
        "                df = pd.read_excel(file_path)\n",
        "            elif file_extension == 'json':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame(json.load(f))\n",
        "            elif file_extension == 'xml':\n",
        "                tree = ET.parse(file_path)\n",
        "                root = tree.getroot()\n",
        "                df = pd.DataFrame(parse_xml(root))\n",
        "            elif file_extension == 'pkl':\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    df = pickle.load(f)\n",
        "            elif file_extension == 'parquet':\n",
        "                df = pd.read_parquet(file_path)\n",
        "            elif file_extension == 'txt':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame({'text': [f.read()]})\n",
        "            elif file_extension in ['jpg', 'jpeg']:\n",
        "                df = pd.DataFrame({'image_path': [file_path]})\n",
        "            elif file_extension in ['mp3', 'wav']:\n",
        "                df = pd.DataFrame({'audio_path': [file_path]})\n",
        "            elif file_extension in ['mp4', 'avi', 'mkv']:\n",
        "                df = pd.DataFrame({'video_path': [file_path]})\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file_extension}\")\n",
        "                continue\n",
        "\n",
        "            dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not dataframes:\n",
        "        print(\"None of the provided files could be loaded\")\n",
        "        raise ValueError(\"None of the provided files could be loaded\")\n",
        "\n",
        "    # Find the union of all columns\n",
        "    standard_columns = set()\n",
        "    for df in dataframes:\n",
        "        standard_columns.update(df.columns)\n",
        "    standard_columns = list(standard_columns)\n",
        "\n",
        "    standardize_dataframes(dataframes, standard_columns)\n",
        "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def parse_xml(root):\n",
        "    data = []\n",
        "    for child in root:\n",
        "        data.append({subchild.tag: subchild.text for subchild in child})\n",
        "    return data\n",
        "\n",
        "def standardize_dataframes(dataframes, standard_columns):\n",
        "    for i, df in enumerate(dataframes):\n",
        "        for col in standard_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None\n",
        "        dataframes[i] = df[standard_columns]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_paths = ['Social_Network_Ads.csv','Walmart_sales.csv']\n",
        "\n",
        "    try:\n",
        "        merged_data = load_data(file_paths)\n",
        "        print(merged_data.head())\n",
        "    except ValueError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g21BbCjWyCJI",
        "outputId": "b0df0411-03c8-4b3d-ec33-7a016960dad0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Weekly_Sales Store  Fuel_Price Age  Gender  Date  Temperature   User ID  \\\n",
            "0           NaN  None         NaN  19    Male  None          NaN  15624510   \n",
            "1           NaN  None         NaN  35    Male  None          NaN  15810944   \n",
            "2           NaN  None         NaN  26  Female  None          NaN  15668575   \n",
            "3           NaN  None         NaN  27  Female  None          NaN  15603246   \n",
            "4           NaN  None         NaN  19    Male  None          NaN  15804002   \n",
            "\n",
            "  Holiday_Flag  Unemployment  CPI Purchased EstimatedSalary  \n",
            "0         None           NaN  NaN         0           19000  \n",
            "1         None           NaN  NaN         0           20000  \n",
            "2         None           NaN  NaN         0           43000  \n",
            "3         None           NaN  NaN         0           57000  \n",
            "4         None           NaN  NaN         0           76000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "# %%\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def load_data(file_paths):\n",
        "    dataframes = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            file_extension = os.path.splitext(file_path)[1][1:].lower()\n",
        "\n",
        "            if file_extension == 'csv':\n",
        "                df = pd.read_csv(file_path)\n",
        "            elif file_extension in ['xls', 'xlsx']:\n",
        "                df = pd.read_excel(file_path)\n",
        "            elif file_extension == 'json':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame(json.load(f))\n",
        "            elif file_extension == 'xml':\n",
        "                tree = ET.parse(file_path)\n",
        "                root = tree.getroot()\n",
        "                df = pd.DataFrame(parse_xml(root))\n",
        "            elif file_extension == 'pkl':\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    df = pickle.load(f)\n",
        "            elif file_extension == 'parquet':\n",
        "                df = pd.read_parquet(file_path)\n",
        "            elif file_extension == 'txt':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame({'text': [f.read()]})\n",
        "            elif file_extension in ['jpg', 'jpeg']:\n",
        "                df = pd.DataFrame({'image_path': [file_path]})\n",
        "            elif file_extension in ['mp3', 'wav']:\n",
        "                df = pd.DataFrame({'audio_path': [file_path]})\n",
        "            elif file_extension in ['mp4', 'avi', 'mkv']:\n",
        "                df = pd.DataFrame({'video_path': [file_path]})\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file_extension}\")\n",
        "                continue\n",
        "\n",
        "            dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not dataframes:\n",
        "        print(\"None of the provided files could be loaded\")\n",
        "        raise ValueError(\"None of the provided files could be loaded\")\n",
        "\n",
        "    # Find the union of all columns\n",
        "    standard_columns = set()\n",
        "    for df in dataframes:\n",
        "        standard_columns.update(df.columns)\n",
        "    standard_columns = list(standard_columns)\n",
        "\n",
        "    standardize_dataframes(dataframes, standard_columns)\n",
        "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def parse_xml(root):\n",
        "    data = []\n",
        "    for child in root:\n",
        "        data.append({subchild.tag: subchild.text for subchild in child})\n",
        "    return data\n",
        "\n",
        "def standardize_dataframes(dataframes, standard_columns):\n",
        "    for i, df in enumerate(dataframes):\n",
        "        # Iterate over columns in standard_columns, but only add those missing\n",
        "        for col in standard_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None  # Add missing columns with None (will be filled later)\n",
        "\n",
        "        # Reorder columns to match standard_columns, filling missing with original values\n",
        "        df = df.reindex(columns=standard_columns, fill_value=df)\n",
        "\n",
        "        dataframes[i] = df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_paths = ['Social_Network_Ads.csv','Walmart_sales.csv']\n",
        "\n",
        "    try:\n",
        "        merged_data = load_data(file_paths)\n",
        "        print(merged_data.head())\n",
        "    except ValueError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1sfQBiWFh_3",
        "outputId": "137b7030-32e4-49b6-a556-48011f79dafe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Weekly_Sales Store  Fuel_Price Age  Gender  Date  Temperature   User ID  \\\n",
            "0           NaN  None         NaN  19    Male  None          NaN  15624510   \n",
            "1           NaN  None         NaN  35    Male  None          NaN  15810944   \n",
            "2           NaN  None         NaN  26  Female  None          NaN  15668575   \n",
            "3           NaN  None         NaN  27  Female  None          NaN  15603246   \n",
            "4           NaN  None         NaN  19    Male  None          NaN  15804002   \n",
            "\n",
            "  Holiday_Flag  Unemployment  CPI Purchased EstimatedSalary  \n",
            "0         None           NaN  NaN         0           19000  \n",
            "1         None           NaN  NaN         0           20000  \n",
            "2         None           NaN  NaN         0           43000  \n",
            "3         None           NaN  NaN         0           57000  \n",
            "4         None           NaN  NaN         0           76000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "# %%\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def load_data(file_paths):\n",
        "    dataframes = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            file_extension = os.path.splitext(file_path)[1][1:].lower()\n",
        "\n",
        "            if file_extension == 'csv':\n",
        "                df = pd.read_csv(file_path)\n",
        "            elif file_extension in ['xls', 'xlsx']:\n",
        "                df = pd.read_excel(file_path)\n",
        "            elif file_extension == 'json':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame(json.load(f))\n",
        "            elif file_extension == 'xml':\n",
        "                tree = ET.parse(file_path)\n",
        "                root = tree.getroot()\n",
        "                df = pd.DataFrame(parse_xml(root))\n",
        "            elif file_extension == 'pkl':\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    df = pickle.load(f)\n",
        "            elif file_extension == 'parquet':\n",
        "                df = pd.read_parquet(file_path)\n",
        "            elif file_extension == 'txt':\n",
        "                with open(file_path, 'r') as f:\n",
        "                    df = pd.DataFrame({'text': [f.read()]})\n",
        "            elif file_extension in ['jpg', 'jpeg']:\n",
        "                df = pd.DataFrame({'image_path': [file_path]})\n",
        "            elif file_extension in ['mp3', 'wav']:\n",
        "                df = pd.DataFrame({'audio_path': [file_path]})\n",
        "            elif file_extension in ['mp4', 'avi', 'mkv']:\n",
        "                df = pd.DataFrame({'video_path': [file_path]})\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file_extension}\")\n",
        "                continue\n",
        "\n",
        "            dataframes.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not dataframes:\n",
        "        print(\"None of the provided files could be loaded\")\n",
        "        raise ValueError(\"None of the provided files could be loaded\")\n",
        "\n",
        "    # Find the union of all columns\n",
        "    standard_columns = set()\n",
        "    for df in dataframes:\n",
        "        standard_columns.update(df.columns)\n",
        "    standard_columns = list(standard_columns)\n",
        "\n",
        "    # Check if standardization would result in NaN or None\n",
        "    all_columns_present = all(all(col in df.columns for df in dataframes) for col in standard_columns)\n",
        "\n",
        "    if all_columns_present:\n",
        "        standardize_dataframes(dataframes, standard_columns)\n",
        "    else:\n",
        "        print(\"Skipping standardization as it would result in NaN or None values.\")\n",
        "\n",
        "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def parse_xml(root):\n",
        "    data = []\n",
        "    for child in root:\n",
        "        data.append({subchild.tag: subchild.text for subchild in child})\n",
        "    return data\n",
        "\n",
        "def standardize_dataframes(dataframes, standard_columns):\n",
        "    for i, df in enumerate(dataframes):\n",
        "        # Iterate over columns in standard_columns, but only add those missing\n",
        "        for col in standard_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = None  # Add missing columns with None (will be filled later)\n",
        "\n",
        "        # Reorder columns to match standard_columns, filling missing with original values\n",
        "        df = df.reindex(columns=standard_columns, fill_value=df)\n",
        "\n",
        "        dataframes[i] = df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_paths = ['Social_Network_Ads.csv','Walmart_sales.csv']\n",
        "\n",
        "    try:\n",
        "        merged_data = load_data(file_paths)\n",
        "        print(merged_data.head())\n",
        "    except ValueError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epeoYFX4F_Pu",
        "outputId": "d91d302f-4cfc-4eed-fb41-94e814a05509"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping standardization as it would result in NaN or None values.\n",
            "      User ID  Gender   Age  EstimatedSalary  Purchased  Store Date  \\\n",
            "0  15624510.0    Male  19.0          19000.0        0.0    NaN  NaN   \n",
            "1  15810944.0    Male  35.0          20000.0        0.0    NaN  NaN   \n",
            "2  15668575.0  Female  26.0          43000.0        0.0    NaN  NaN   \n",
            "3  15603246.0  Female  27.0          57000.0        0.0    NaN  NaN   \n",
            "4  15804002.0    Male  19.0          76000.0        0.0    NaN  NaN   \n",
            "\n",
            "   Weekly_Sales  Holiday_Flag  Temperature  Fuel_Price  CPI  Unemployment  \n",
            "0           NaN           NaN          NaN         NaN  NaN           NaN  \n",
            "1           NaN           NaN          NaN         NaN  NaN           NaN  \n",
            "2           NaN           NaN          NaN         NaN  NaN           NaN  \n",
            "3           NaN           NaN          NaN         NaN  NaN           NaN  \n",
            "4           NaN           NaN          NaN         NaN  NaN           NaN  \n"
          ]
        }
      ]
    }
  ]
}